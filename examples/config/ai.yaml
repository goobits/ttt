# AI Library Configuration Example
# Copy this to ~/.config/ai/config.yaml or ./ai.yaml

# Default backend selection
# Options: auto, local, cloud, or specific backend name
default_backend: auto

# Default model selection
# Can be a model name, alias, or null for automatic selection
default_model: null

# Request timeout in seconds
timeout: 30

# Maximum number of retries for failed requests
max_retries: 3

# Enable automatic fallback to other backends
enable_fallbacks: true

# Order in which to try backends when fallback is enabled
fallback_order:
  - cloud
  - local

# Local backend configuration (Ollama)
ollama_base_url: http://localhost:11434

# Model aliases for convenience
# Use these in place of full model names
model_aliases:
  # Speed-optimized models
  fast: gpt-3.5-turbo
  quick: gpt-3.5-turbo
  
  # Quality-optimized models
  best: gpt-4
  quality: gpt-4
  smart: gpt-4
  
  # Task-specific models
  coding: claude-3-sonnet
  code: claude-3-sonnet
  analysis: claude-3-sonnet
  
  # Cost-optimized models
  cheap: gpt-3.5-turbo
  affordable: gpt-3.5-turbo
  
  # Privacy-focused models (local)
  local: llama2
  private: llama2
  offline: llama2

# Custom model definitions
# Add your own models here
models:
  # Example: Custom OpenAI model
  - name: gpt-4-turbo
    provider: openai
    provider_name: gpt-4-turbo-preview
    aliases:
      - turbo
      - preview
    speed: medium
    quality: high
    capabilities:
      - text
      - reasoning
      - code
    context_length: 128000
    cost_per_token: 0.00001
  
  # Example: Custom local model
  - name: codellama
    provider: local
    provider_name: codellama:13b
    aliases:
      - local-code
      - code-local
    speed: medium
    quality: medium
    capabilities:
      - code
      - text
    context_length: 4096
  
  # Example: Fine-tuned model
  - name: my-custom-model
    provider: openai
    provider_name: ft:gpt-3.5-turbo:my-org:custom:abc123
    aliases:
      - custom
      - tuned
    speed: fast
    quality: high
    capabilities:
      - text
      - domain-specific

# Backend-specific configurations
backends:
  local:
    # Ollama-specific settings
    default_model: llama2
    timeout: 60
    # Add any other Ollama-specific options here
  
  cloud:
    # Cloud backend settings
    default_model: gpt-3.5-turbo
    timeout: 30
    # Provider preferences for cloud backend
    provider_order:
      - openai
      - anthropic
      - google

# Smart routing configuration
routing:
  # Keywords that suggest code-related queries
  code_keywords:
    - "code"
    - "function"
    - "debug"
    - "implement"
    - "python"
    - "javascript"
    - "fix"
    - "error"
    - "syntax"
  
  # Keywords that suggest fast response needed
  speed_keywords:
    - "quick"
    - "fast"
    - "simple"
    - "brief"
    - "summary"
    - "tldr"
  
  # Keywords that suggest quality/complex response needed
  quality_keywords:
    - "analyze"
    - "explain"
    - "detailed"
    - "comprehensive"
    - "thorough"
    - "complex"
    - "research"

# Logging configuration
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR
  format: simple  # simple, detailed
  file: null  # Path to log file, or null for console only

# Advanced options
advanced:
  # Enable response caching
  enable_cache: false
  cache_ttl: 3600  # Cache time-to-live in seconds
  
  # Enable usage tracking
  track_usage: false
  usage_file: ~/.config/ai/usage.json
  
  # Enable model performance monitoring
  track_performance: false
  performance_file: ~/.config/ai/performance.json