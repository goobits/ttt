# Testing Guide

This document provides comprehensive guidance for testing the TTT library, including test structure, patterns, rate limiting, and best practices.

## Test Structure

The testing suite uses pytest with comprehensive coverage across all components:

### Core Components

#### 1. API Testing (`test_api_*.py`)
- **Core API** (`test_api_core.py`): Basic ask/stream/chat functionality
- **Streaming API** (`test_api_streaming.py`): Streaming responses and async operations
- **Response objects**: AIResponse, error handling, metadata validation

#### 2. Backend Testing (`test_backends_*.py`)
- **Cloud Backend** (`test_backends_cloud.py`): LiteLLM integration, provider routing
- **Local Backend** (`test_backends_local.py`): Ollama integration with mocked HTTP calls
- **Error Handling**: Rate limits, authentication, model availability

#### 3. CLI Testing (`test_cli_modern.py`)
- **Click Framework**: Modern Click-based CLI with rich-click styling
- **Command Testing**: All commands (ask, chat, list, config, tools, status, models, info, export)
- **Hook System**: Integration with cli_handlers.py for command execution
- **Help & Validation**: Help text, option parsing, error handling

#### 4. Tool System (`test_tools_*.py`)
- **Built-in Tools** (`test_tools_builtin.py`): calculate, web_search, code_execution, file operations
- **Custom Tools** (`test_tools_custom.py`): Tool registration and decoration patterns
- **Tool Integration** (`test_tools_chat.py`): Tool usage in chat sessions

#### 5. Configuration & Routing
- **Configuration** (`test_config_loading.py`): YAML loading, environment variables, hierarchy
- **Model Routing** (`test_routing.py`): Provider selection, model registry, fallbacks
- **Models** (`test_models.py`): Model definitions, capabilities, metadata

## Running Tests

### Using the Test Script (Recommended)

**Always use `./test.sh` for tests** - DO NOT run `pytest` directly for integration tests as it bypasses critical rate limiting.

```bash
# Run unit tests (fast, no API calls)
./test.sh                      # Default: run unit tests
./test.sh unit                 # Explicit unit tests
./test.sh unit --coverage      # With coverage report

# Run integration tests (requires API keys, costs money)
export OPENROUTER_API_KEY=your-key-here
./test.sh integration          # Will prompt for confirmation
./test.sh integration --force  # Skip confirmation

# Run all tests
./test.sh all                  # Unit tests first, then integration

# Run specific test files
./test.sh --test test_cli_modern.py

# Skip slow tests
./test.sh --markers "not slow"

# Verbose output
./test.sh unit --verbose
```

### Direct pytest Usage (Unit Tests Only)

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_cli_modern.py -v

# Run with markers
pytest tests/ -m "not integration"  # Skip integration tests
pytest tests/ -m "not slow"         # Skip slow tests

# Run with coverage
pytest tests/ --cov=ttt --cov-report=html
```

### Test Categories

Tests are marked with these pytest markers:

- `unit`: Fast tests with mocking, no external dependencies
- `integration`: Real API calls, requires API keys and costs money
- `slow`: Time-intensive tests
- `asyncio`: Async test functions

## Rate Limiting for Integration Tests

To prevent hitting API rate limits, we provide several pytest fixtures that automatically add delays between API calls.

### Default Rate Limits

- **OpenRouter**: 1.0 second delay
- **OpenAI**: 0.5 second delay
- **Anthropic**: 0.5 second delay
- **Unknown providers**: 1.0 second delay

### Available Fixtures

#### 1. `delayed_ask`
Wrapper around `ask()` with automatic delays:

```python
def test_with_delayed_ask(delayed_ask):
    response = delayed_ask("Hello", model="gpt-3.5-turbo")
    assert response.succeeded
```

#### 2. `delayed_stream`
Wrapper around `stream()` with automatic delays:

```python
def test_with_delayed_stream(delayed_stream):
    chunks = []
    for chunk in delayed_stream("Count to 3", model="gpt-3.5-turbo"):
        chunks.append(chunk)
    assert len(chunks) > 0
```

#### 3. `delayed_chat`
Wrapper around `chat()` for session-based conversations:

```python
def test_with_delayed_chat(delayed_chat):
    with delayed_chat(model="gpt-3.5-turbo") as session:
        response1 = session.ask("My name is Alice")
        response2 = session.ask("What's my name?")
        assert "Alice" in str(response2)
```

#### 4. `rate_limit_delay`
Manual delay function:

```python
def test_with_manual_delay(rate_limit_delay, delayed_ask):
    response = delayed_ask("Hello")
    
    # Add extra delay for stress testing
    rate_limit_delay("openrouter")
    
    response2 = delayed_ask("World")
```

### Configuration

#### Environment Variables

```bash
# Set custom delays for each provider
export OPENROUTER_RATE_DELAY=2.0  # 2 seconds
export OPENAI_RATE_DELAY=1.0      # 1 second
export ANTHROPIC_RATE_DELAY=0.3   # 0.3 seconds

# Run tests with custom delays
pytest tests/test_integration.py
```

#### Command Line

```bash
# Set global rate delay for all providers
pytest tests/test_integration.py --rate-delay 1.5
```

## Testing Patterns

### CLI Testing Pattern

```python
from click.testing import CliRunner
from ttt.cli import main

def test_ask_command():
    """Test the modern Click-based CLI."""
    runner = CliRunner()

    with patch("ttt.cli_handlers.on_ask") as mock_hook:
        result = runner.invoke(main, ["ask", "What is Python?"])

        assert result.exit_code == 0
        mock_hook.assert_called_once()
```

### Backend Testing Pattern

```python
@pytest.mark.asyncio
async def test_local_backend():
    """Test LocalBackend with mocked httpx."""
    with patch("httpx.AsyncClient") as mock_client_class:
        mock_response = Mock()
        mock_response.json.return_value = {"response": "Hello!"}

        mock_client = AsyncMock()
        mock_client.post = AsyncMock(return_value=mock_response)
        mock_client_class.return_value.__aenter__ = AsyncMock(return_value=mock_client)

        backend = LocalBackend()
        response = await backend.ask("Hello, AI!")
        assert str(response) == "Hello!"
```

### Tool Testing Pattern

```python
def test_calculate_tool_security():
    """Test that unsafe operations are blocked."""
    from ttt.tools.builtins import calculate

    # Valid operation
    result = calculate("2 + 2")
    assert "Result: 4" in result

    # Security: unsafe operations should be blocked
    result = calculate("__import__('os')")
    assert "Error" in result

    # Edge case: division by zero
    result = calculate("1 / 0")
    assert "Error: Division by zero" in result
```

### Integration Testing Pattern

```python
@pytest.mark.integration
def test_multiple_providers(delayed_ask):
    """Test multiple providers with appropriate delays."""
    providers = [
        ("openrouter/google/gemini-2.5-flash", "OpenRouter test"),
        ("gpt-3.5-turbo", "OpenAI test"),
        ("claude-3-haiku-20240307", "Anthropic test")
    ]

    for model, prompt in providers:
        response = delayed_ask(prompt, model=model)
        assert response.succeeded
```

## Best Practices

### 1. Test Isolation
- Use mocking to isolate units under test
- Patch external dependencies (HTTP, file system, etc.)
- Each test should be independent and repeatable

### 2. Comprehensive Edge Cases
- Test valid inputs, invalid inputs, edge cases, error conditions
- Include security testing for code execution and file operations
- Test rate limiting, authentication failures, network errors

### 3. CLI Testing
- Use Click's CliRunner for reliable CLI testing
- Mock app hooks to test CLI integration without side effects
- Test help text, option parsing, command validation

### 4. Async Testing
- Use `@pytest.mark.asyncio` for async test functions
- Use `AsyncMock` for async dependencies
- Proper async context manager mocking with `__aenter__`

### 5. Integration Testing
- Mark with `@pytest.mark.integration`
- Use rate limiting fixtures (`delayed_ask`, etc.)
- Require explicit API keys and cost acknowledgment
- Monitor for rate limit errors even with delays

## File Organization

```
tests/
├── conftest.py                 # Pytest configuration and fixtures
├── test_api_*.py              # API functionality tests
├── test_backends_*.py         # Backend implementation tests
├── test_cli_modern.py         # Modern Click CLI tests
├── test_tools_*.py            # Tool system tests
├── test_config_loading.py     # Configuration tests
├── test_*_streaming.py        # Streaming functionality tests
├── test_integration.py        # Real API integration tests
└── README.md                  # This file
```

## Writing New Tests

### Example: Adding a New Feature Test

```python
import pytest
from unittest.mock import patch, Mock
from ttt import ask

class TestNewFeature:
    """Test suite for new feature."""
    
    @pytest.mark.unit
    def test_basic_functionality(self):
        """Test basic feature behavior."""
        with patch("ttt.backends.cloud.CloudBackend.ask") as mock_ask:
            mock_ask.return_value = Mock(text="Response")
            result = my_new_feature("input")
            assert result == expected_value
    
    @pytest.mark.integration
    def test_real_api_call(self, delayed_ask):
        """Test with real API (costs money)."""
        response = delayed_ask("Test new feature")
        assert response.succeeded
    
    @pytest.mark.parametrize("input,expected", [
        ("case1", "result1"),
        ("case2", "result2"),
        ("edge_case", "edge_result"),
    ])
    def test_various_inputs(self, input, expected):
        """Test multiple input scenarios."""
        result = my_new_feature(input)
        assert result == expected
```

## Troubleshooting

### Common Issues

**Rate Limit Errors**
```bash
# Increase delays
export OPENROUTER_RATE_DELAY=3.0

# Or use command line option
pytest tests/test_integration.py --rate-delay 2.5
```

**Missing API Keys**
```bash
# Check environment
echo $OPENAI_API_KEY

# Set for session
export OPENAI_API_KEY=sk-...
```

**Slow Tests**
```bash
# Skip slow tests
./test.sh --markers "not slow"

# Reduce delays if you have higher limits
export OPENAI_RATE_DELAY=0.1
```

## Dependencies

### Required (in pyproject.toml)
- `pytest ^7.0.0`
- `pytest-asyncio ^0.21.0`
- `pytest-cov ^4.0.0`

### CLI Testing
- `click` - CLI framework
- `rich-click` - Enhanced Click with rich formatting

## Continuous Integration

Tests are automatically run on:
- Every push to main branch
- Every pull request
- Can be manually triggered

CI configuration ensures:
- All unit tests pass
- Code coverage meets minimum threshold
- Linting and formatting checks pass
- Type checking passes